# Perceptron and Adaline from Scratch

This repository contains an implementation of two classic machine learning algorithmsâ€”**Perceptron** and **Adaline (Adaptive Linear Neuron)**â€”built from scratch using Python and NumPy.

## ðŸš€ Whatâ€™s Inside

- `Perceptron`: A binary classifier that learns using a simple update rule based on misclassifications.
- `Adaline`: A linear classifier that minimizes the squared error cost function using gradient descent.
- Comparison of **learning rates** for Adaline and their impact on convergence using loss plots.

## ðŸ§  Algorithms

### Perceptron

- Initialized with a learning rate, number of iterations, and random seed
- Updates weights based on classification errors
- Tracks errors over each epoch

### Adaline (Gradient Descent)

- Uses continuous output (net input) rather than binary step function
- Implements batch gradient descent
- Tracks **mean squared error** per epoch
- Plotted convergence for different learning rates

## ðŸ“Š Visualization

Loss was plotted for two Adaline models with different learning rates:
- `ada1` with learning rate `0.1`
- `ada2` with learning rate `0.001`

This helps visualize how learning rate affects convergence.

## ðŸ“¦ Dependencies

- `numpy`
- `matplotlib`

You can install them using:

```bash
pip install numpy matplotlib
